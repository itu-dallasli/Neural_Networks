{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.transforms import v2\nfrom torchvision.io import read_image\nimport numpy as np\nfrom torch.utils.data import Subset","metadata":{"id":"GG9TtmCp14KC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"id":"dn9zyiSL8WPN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32","metadata":{"id":"-X081gG0Nv68","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = datasets.CIFAR10(\n    root=\"data\",\n    train=True,\n    transform=preprocess,\n    download=True\n)\n\ntest_data = datasets.CIFAR10(\n    root=\"data\",\n    train=False,\n    transform=preprocess,\n    download=True\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)","metadata":{"id":"NJ0kwq047As-","outputId":"d85856cc-6186-405f-df03-0ed7613cadbe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models","metadata":{"id":"uDdSe4fW2EH1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torchvision.models.resnet18()","metadata":{"id":"FWPiPJ5z2FVK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\nprint(device)","metadata":{"id":"MVMC3qi9yYZJ","outputId":"21dce2f3-fea0-480d-c8a7-dc3f92583bcb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"id":"6K7jg5Q1ycs5","outputId":"a12f59ac-11d2-41fb-b011-8db64dd5065a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"initial_subset_size = 3125  # Starting with N samples\nincrease_factor = 2  # Increase the subset size by this factor\nepochs = 25  # Total number of epochs\nincrease_interval = 5  # Number of epochs before increasing the subset size\nlearning_rate = 1e-3\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"id":"gpUvtUnA2tgW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef create_stratified_subset_dataloader(dataset, subset_size):\n    targets = dataset.targets\n    indices = np.arange(len(targets))\n    subset_indices, _ = train_test_split(indices, train_size=subset_size, stratify=targets)\n    subset = Subset(dataset, subset_indices)\n    return DataLoader(subset, batch_size=batch_size, shuffle=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        X = X.to(device)\n        y = y.to(device)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n    with torch.no_grad():\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"id":"EiuQDf91235c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"current_subset_size = initial_subset_size\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_dataloader = create_stratified_subset_dataloader(training_data, current_subset_size) \n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\n\n    # Increase the subset size at specified intervals\n    if (t + 1) % increase_interval == 0 and current_subset_size * increase_factor <= len(training_data):\n        current_subset_size *= increase_factor\n        if current_subset_size == 50000: current_subset_size = 25000\n\nprint(\"Done!\")","metadata":{"id":"_5lQnUh_24hI","outputId":"abfa2010-6503-4ba4-8015-c56d4800882f","trusted":true},"execution_count":null,"outputs":[]}]}