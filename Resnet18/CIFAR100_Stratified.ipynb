{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor\nfrom torchvision.transforms import v2\nfrom torchvision.io import read_image\nimport numpy as np\nfrom torch.utils.data import Subset","metadata":{"id":"GG9TtmCp14KC","execution":{"iopub.status.busy":"2024-06-09T18:46:49.989875Z","iopub.execute_input":"2024-06-09T18:46:49.990597Z","iopub.status.idle":"2024-06-09T18:46:49.996035Z","shell.execute_reply.started":"2024-06-09T18:46:49.990564Z","shell.execute_reply":"2024-06-09T18:46:49.995128Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"preprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"id":"dn9zyiSL8WPN","execution":{"iopub.status.busy":"2024-06-09T18:46:49.997723Z","iopub.execute_input":"2024-06-09T18:46:49.997989Z","iopub.status.idle":"2024-06-09T18:46:50.008024Z","shell.execute_reply.started":"2024-06-09T18:46:49.997958Z","shell.execute_reply":"2024-06-09T18:46:50.007234Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"batch_size = 32","metadata":{"id":"-X081gG0Nv68","execution":{"iopub.status.busy":"2024-06-09T18:46:50.009631Z","iopub.execute_input":"2024-06-09T18:46:50.009906Z","iopub.status.idle":"2024-06-09T18:46:50.016334Z","shell.execute_reply.started":"2024-06-09T18:46:50.009883Z","shell.execute_reply":"2024-06-09T18:46:50.015575Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"training_data = datasets.CIFAR100(\n    root=\"data\",\n    train=True,\n    transform=preprocess,\n    download=True\n)\n\ntest_data = datasets.CIFAR100(\n    root=\"data\",\n    train=False,\n    transform=preprocess,\n    download=True\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)","metadata":{"id":"NJ0kwq047As-","outputId":"d85856cc-6186-405f-df03-0ed7613cadbe","execution":{"iopub.status.busy":"2024-06-09T18:46:50.017321Z","iopub.execute_input":"2024-06-09T18:46:50.017629Z","iopub.status.idle":"2024-06-09T18:46:51.835597Z","shell.execute_reply.started":"2024-06-09T18:46:50.017599Z","shell.execute_reply":"2024-06-09T18:46:51.834791Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchvision.models","metadata":{"id":"uDdSe4fW2EH1","execution":{"iopub.status.busy":"2024-06-09T18:46:51.838042Z","iopub.execute_input":"2024-06-09T18:46:51.838332Z","iopub.status.idle":"2024-06-09T18:46:51.842767Z","shell.execute_reply.started":"2024-06-09T18:46:51.838307Z","shell.execute_reply":"2024-06-09T18:46:51.841687Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model = torchvision.models.resnet18()","metadata":{"id":"FWPiPJ5z2FVK","execution":{"iopub.status.busy":"2024-06-09T18:46:51.843815Z","iopub.execute_input":"2024-06-09T18:46:51.844044Z","iopub.status.idle":"2024-06-09T18:46:52.051805Z","shell.execute_reply.started":"2024-06-09T18:46:51.844024Z","shell.execute_reply":"2024-06-09T18:46:52.051008Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\nprint(device)","metadata":{"id":"MVMC3qi9yYZJ","outputId":"21dce2f3-fea0-480d-c8a7-dc3f92583bcb","execution":{"iopub.status.busy":"2024-06-09T18:46:52.052986Z","iopub.execute_input":"2024-06-09T18:46:52.053301Z","iopub.status.idle":"2024-06-09T18:46:52.058025Z","shell.execute_reply.started":"2024-06-09T18:46:52.053274Z","shell.execute_reply":"2024-06-09T18:46:52.056988Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to(device)","metadata":{"id":"6K7jg5Q1ycs5","outputId":"a12f59ac-11d2-41fb-b011-8db64dd5065a","execution":{"iopub.status.busy":"2024-06-09T18:46:52.059359Z","iopub.execute_input":"2024-06-09T18:46:52.059661Z","iopub.status.idle":"2024-06-09T18:46:52.084635Z","shell.execute_reply.started":"2024-06-09T18:46:52.059638Z","shell.execute_reply":"2024-06-09T18:46:52.083803Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"initial_subset_size = 3125  # Starting with N samples\nincrease_factor = 2  # Increase the subset size by this factor\nepochs = 25  # Total number of epochs\nincrease_interval = 5  # Number of epochs before increasing the subset size\nlearning_rate = 1e-3\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"id":"gpUvtUnA2tgW","execution":{"iopub.status.busy":"2024-06-09T18:46:52.086014Z","iopub.execute_input":"2024-06-09T18:46:52.086318Z","iopub.status.idle":"2024-06-09T18:46:52.092391Z","shell.execute_reply.started":"2024-06-09T18:46:52.086289Z","shell.execute_reply":"2024-06-09T18:46:52.091472Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef create_stratified_subset_dataloader(dataset, subset_size):\n    targets = dataset.targets\n    indices = np.arange(len(targets))\n    subset_indices, _ = train_test_split(indices, train_size=subset_size, stratify=targets)\n    subset = Subset(dataset, subset_indices)\n    return DataLoader(subset, batch_size=batch_size, shuffle=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-09T18:46:52.093690Z","iopub.execute_input":"2024-06-09T18:46:52.094009Z","iopub.status.idle":"2024-06-09T18:46:52.100480Z","shell.execute_reply.started":"2024-06-09T18:46:52.093976Z","shell.execute_reply":"2024-06-09T18:46:52.099708Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    # Set the model to training mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        X = X.to(device)\n        y = y.to(device)\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * batch_size + len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    # Set the model to evaluation mode - important for batch normalization and dropout layers\n    # Unnecessary in this situation but added for best practices\n    model.eval()\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n    with torch.no_grad():\n        for X, y in dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","metadata":{"id":"EiuQDf91235c","execution":{"iopub.status.busy":"2024-06-09T18:46:52.102901Z","iopub.execute_input":"2024-06-09T18:46:52.103191Z","iopub.status.idle":"2024-06-09T18:46:52.113337Z","shell.execute_reply.started":"2024-06-09T18:46:52.103166Z","shell.execute_reply":"2024-06-09T18:46:52.112272Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"current_subset_size = initial_subset_size\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_dataloader = create_stratified_subset_dataloader(training_data, current_subset_size) \n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\n\n    # Increase the subset size at specified intervals\n    if (t + 1) % increase_interval == 0 and current_subset_size * increase_factor <= len(training_data):\n        current_subset_size *= increase_factor\n        if current_subset_size == 50000: current_subset_size = 25000\n\nprint(\"Done!\")","metadata":{"id":"_5lQnUh_24hI","outputId":"abfa2010-6503-4ba4-8015-c56d4800882f","execution":{"iopub.status.busy":"2024-06-09T18:46:52.114762Z","iopub.execute_input":"2024-06-09T18:46:52.115119Z","iopub.status.idle":"2024-06-09T19:14:13.268991Z","shell.execute_reply.started":"2024-06-09T18:46:52.115089Z","shell.execute_reply":"2024-06-09T19:14:13.268088Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nloss: 6.977133  [   32/ 3125]\nTest Error: \n Accuracy: 4.9%, Avg loss: 4.453303 \n\nEpoch 2\n-------------------------------\nloss: 4.422045  [   32/ 3125]\nTest Error: \n Accuracy: 5.2%, Avg loss: 4.388811 \n\nEpoch 3\n-------------------------------\nloss: 4.309286  [   32/ 3125]\nTest Error: \n Accuracy: 6.5%, Avg loss: 4.089954 \n\nEpoch 4\n-------------------------------\nloss: 3.956225  [   32/ 3125]\nTest Error: \n Accuracy: 6.8%, Avg loss: 4.025901 \n\nEpoch 5\n-------------------------------\nloss: 4.067620  [   32/ 3125]\nTest Error: \n Accuracy: 9.4%, Avg loss: 3.952619 \n\nEpoch 6\n-------------------------------\nloss: 4.077469  [   32/ 6250]\nloss: 3.993210  [ 3232/ 6250]\nTest Error: \n Accuracy: 9.8%, Avg loss: 3.921487 \n\nEpoch 7\n-------------------------------\nloss: 3.687742  [   32/ 6250]\nloss: 3.564697  [ 3232/ 6250]\nTest Error: \n Accuracy: 11.9%, Avg loss: 3.764094 \n\nEpoch 8\n-------------------------------\nloss: 3.521296  [   32/ 6250]\nloss: 3.519873  [ 3232/ 6250]\nTest Error: \n Accuracy: 13.2%, Avg loss: 3.728837 \n\nEpoch 9\n-------------------------------\nloss: 3.591866  [   32/ 6250]\nloss: 3.751901  [ 3232/ 6250]\nTest Error: \n Accuracy: 14.3%, Avg loss: 3.608156 \n\nEpoch 10\n-------------------------------\nloss: 3.517486  [   32/ 6250]\nloss: 3.222846  [ 3232/ 6250]\nTest Error: \n Accuracy: 17.5%, Avg loss: 3.412174 \n\nEpoch 11\n-------------------------------\nloss: 3.134716  [   32/12500]\nloss: 3.578354  [ 3232/12500]\nloss: 3.419350  [ 6432/12500]\nloss: 3.156014  [ 9632/12500]\nTest Error: \n Accuracy: 20.0%, Avg loss: 3.317550 \n\nEpoch 12\n-------------------------------\nloss: 3.797380  [   32/12500]\nloss: 3.383489  [ 3232/12500]\nloss: 3.026726  [ 6432/12500]\nloss: 3.756833  [ 9632/12500]\nTest Error: \n Accuracy: 25.4%, Avg loss: 3.019899 \n\nEpoch 13\n-------------------------------\nloss: 3.059246  [   32/12500]\nloss: 2.621199  [ 3232/12500]\nloss: 2.623664  [ 6432/12500]\nloss: 2.728886  [ 9632/12500]\nTest Error: \n Accuracy: 27.4%, Avg loss: 2.864540 \n\nEpoch 14\n-------------------------------\nloss: 2.607551  [   32/12500]\nloss: 2.579696  [ 3232/12500]\nloss: 2.945359  [ 6432/12500]\nloss: 3.287369  [ 9632/12500]\nTest Error: \n Accuracy: 29.3%, Avg loss: 2.762661 \n\nEpoch 15\n-------------------------------\nloss: 3.202982  [   32/12500]\nloss: 2.228670  [ 3232/12500]\nloss: 2.788839  [ 6432/12500]\nloss: 2.635523  [ 9632/12500]\nTest Error: \n Accuracy: 32.1%, Avg loss: 2.633067 \n\nEpoch 16\n-------------------------------\nloss: 2.528197  [   32/25000]\nloss: 2.796864  [ 3232/25000]\nloss: 2.034860  [ 6432/25000]\nloss: 2.713406  [ 9632/25000]\nloss: 2.591038  [12832/25000]\nloss: 2.731102  [16032/25000]\nloss: 2.578676  [19232/25000]\nloss: 2.524610  [22432/25000]\nTest Error: \n Accuracy: 38.4%, Avg loss: 2.364132 \n\nEpoch 17\n-------------------------------\nloss: 2.276677  [   32/25000]\nloss: 1.994975  [ 3232/25000]\nloss: 2.037935  [ 6432/25000]\nloss: 1.909255  [ 9632/25000]\nloss: 1.904932  [12832/25000]\nloss: 2.010026  [16032/25000]\nloss: 1.963572  [19232/25000]\nloss: 2.374540  [22432/25000]\nTest Error: \n Accuracy: 42.6%, Avg loss: 2.142695 \n\nEpoch 18\n-------------------------------\nloss: 1.641417  [   32/25000]\nloss: 1.955595  [ 3232/25000]\nloss: 1.967707  [ 6432/25000]\nloss: 2.015899  [ 9632/25000]\nloss: 1.924481  [12832/25000]\nloss: 1.664589  [16032/25000]\nloss: 1.750826  [19232/25000]\nloss: 2.061872  [22432/25000]\nTest Error: \n Accuracy: 44.5%, Avg loss: 2.093328 \n\nEpoch 19\n-------------------------------\nloss: 1.678176  [   32/25000]\nloss: 1.848472  [ 3232/25000]\nloss: 2.197066  [ 6432/25000]\nloss: 1.686295  [ 9632/25000]\nloss: 2.196124  [12832/25000]\nloss: 1.587184  [16032/25000]\nloss: 1.910923  [19232/25000]\nloss: 1.615501  [22432/25000]\nTest Error: \n Accuracy: 48.7%, Avg loss: 1.865854 \n\nEpoch 20\n-------------------------------\nloss: 1.427927  [   32/25000]\nloss: 1.674043  [ 3232/25000]\nloss: 2.405941  [ 6432/25000]\nloss: 1.171335  [ 9632/25000]\nloss: 1.225323  [12832/25000]\nloss: 1.713477  [16032/25000]\nloss: 1.430272  [19232/25000]\nloss: 1.243862  [22432/25000]\nTest Error: \n Accuracy: 50.4%, Avg loss: 1.839367 \n\nEpoch 21\n-------------------------------\nloss: 1.259976  [   32/25000]\nloss: 1.308155  [ 3232/25000]\nloss: 1.463158  [ 6432/25000]\nloss: 1.292690  [ 9632/25000]\nloss: 1.324948  [12832/25000]\nloss: 1.688452  [16032/25000]\nloss: 1.185242  [19232/25000]\nloss: 1.330083  [22432/25000]\nTest Error: \n Accuracy: 53.3%, Avg loss: 1.748435 \n\nEpoch 22\n-------------------------------\nloss: 0.827516  [   32/25000]\nloss: 1.147217  [ 3232/25000]\nloss: 1.100006  [ 6432/25000]\nloss: 1.333996  [ 9632/25000]\nloss: 1.164687  [12832/25000]\nloss: 1.142477  [16032/25000]\nloss: 1.443556  [19232/25000]\nloss: 0.975616  [22432/25000]\nTest Error: \n Accuracy: 53.9%, Avg loss: 1.704737 \n\nEpoch 23\n-------------------------------\nloss: 1.117403  [   32/25000]\nloss: 0.969189  [ 3232/25000]\nloss: 0.800235  [ 6432/25000]\nloss: 0.724439  [ 9632/25000]\nloss: 0.968645  [12832/25000]\nloss: 0.650830  [16032/25000]\nloss: 1.141426  [19232/25000]\nloss: 1.398344  [22432/25000]\nTest Error: \n Accuracy: 52.7%, Avg loss: 1.761704 \n\nEpoch 24\n-------------------------------\nloss: 0.780840  [   32/25000]\nloss: 0.803081  [ 3232/25000]\nloss: 0.723103  [ 6432/25000]\nloss: 0.919254  [ 9632/25000]\nloss: 1.014618  [12832/25000]\nloss: 0.782695  [16032/25000]\nloss: 1.379426  [19232/25000]\nloss: 0.891941  [22432/25000]\nTest Error: \n Accuracy: 54.8%, Avg loss: 1.752628 \n\nEpoch 25\n-------------------------------\nloss: 0.826868  [   32/25000]\nloss: 1.099567  [ 3232/25000]\nloss: 0.524196  [ 6432/25000]\nloss: 0.659926  [ 9632/25000]\nloss: 1.027486  [12832/25000]\nloss: 0.812467  [16032/25000]\nloss: 1.130949  [19232/25000]\nloss: 1.135809  [22432/25000]\nTest Error: \n Accuracy: 55.1%, Avg loss: 1.787075 \n\nDone!\n","output_type":"stream"}]}]}